{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying to AWS\n",
    "\n",
    "This notebook contains scripts that we will deploy to AWS, starting with the original dataset already in an S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "\n",
    "code_path = 'src/'\n",
    "os.makedirs(code_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "etl_path = os.path.join(code_path, 'etl/')\n",
    "os.makedirs(etl_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/etl/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/etl/script.py\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import boto3\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'INPUT_BUCKET', 'INPUT_KEY', 'OUTPUT_BUCKET', 'OUTPUT_KEY'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Read data from S3\n",
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=args['INPUT_BUCKET'], Key=args['INPUT_KEY'])\n",
    "df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
    "\n",
    "# Perform transformations\n",
    "df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n",
    "df['GameDifficulty'] = df['GameDifficulty'].map({'Easy': 0, 'Medium': 1, 'Hard': 2})\n",
    "df_encoded = pd.get_dummies(df, columns=['Location', 'GameGenre'], drop_first=True)\n",
    "\n",
    "# Convert the DataFrame back to CSV\n",
    "csv_buffer = StringIO()\n",
    "df_encoded.to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Upload the transformed data to S3\n",
    "s3_client.put_object(Bucket=args['OUTPUT_BUCKET'], Key=args['OUTPUT_KEY'], Body=csv_buffer.getvalue())\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script uploaded to s3://gaming-behavior/glue-scripts/script.py\n"
     ]
    }
   ],
   "source": [
    "file_path = os.path.join(etl_path, 'script.py')\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = 'gaming-behavior'\n",
    "script_file_name = 'script.py'\n",
    "s3_key = f'glue-scripts/{script_file_name}'\n",
    "\n",
    "# Upload the script to S3\n",
    "s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "print(f'Script uploaded to s3://{bucket_name}/{s3_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue job my-glue-job created successfully\n"
     ]
    }
   ],
   "source": [
    "glue_client = boto3.client('glue')\n",
    "\n",
    "# Parameters for the Glue job\n",
    "job_name = 'my-glue-job'\n",
    "role =   # Replace this with your actual IAM role ARN or name\n",
    "script_location = f's3://{bucket_name}/{s3_key}'\n",
    "\n",
    "# S3 locations for input and output data\n",
    "input_bucket = 'gaming-behavior'\n",
    "input_key = 'online_gaming_behavior_dataset.csv'\n",
    "output_bucket = 'gaming-behavior'\n",
    "output_key = 'transformed_online_gaming_behavior_dataset.csv'  # Change as needed\n",
    "\n",
    "# Create or update the Glue job\n",
    "response = glue_client.create_job(\n",
    "    Name=job_name,\n",
    "    Role=role,\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': script_location,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python',\n",
    "        '--enable-continuous-cloudwatch-log': 'true',\n",
    "        '--enable-spark-ui': 'true',\n",
    "        '--INPUT_BUCKET': input_bucket,\n",
    "        '--INPUT_KEY': input_key,\n",
    "        '--OUTPUT_BUCKET': output_bucket,\n",
    "        '--OUTPUT_KEY': output_key\n",
    "    },\n",
    "    MaxRetries=0,\n",
    "    MaxCapacity=2.0,\n",
    "    Timeout=2880,\n",
    "    GlueVersion='2.0'\n",
    ")\n",
    "\n",
    "print(f'Glue job {job_name} created successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue job my-glue-job started successfully with run ID: jr_aa0cc94d4381777fe55bafdf04ed49e708a1ff78fb57423a246be6d631c84165\n"
     ]
    }
   ],
   "source": [
    "start_response = glue_client.start_job_run(JobName=job_name)\n",
    "print(f'Glue job {job_name} started successfully with run ID: {start_response[\"JobRunId\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "sess = sagemaker.Session()\n",
    "region = sess.boto_session.region_name\n",
    "bucket = 'sagemaker-mini'\n",
    "print(\"Using bucket\" + bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_prefix = \"sagemaker/mobile_price_classification/sklearncontainer\"\n",
    "trainpath = sess.upload_data(\n",
    "    path=\"train-V-1.csv\", bucket=bucket, key_prefix=sk_prefix\n",
    ")\n",
    "\n",
    "testpath = sess.upload_data(\n",
    "    path=\"test-V-1.csv\", bucket=bucket, key_prefix=sk_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%writefile script.py\n",
    "\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "import sklearn\n",
    "import joblib\n",
    "import boto3\n",
    "import pathlib\n",
    "from io import  StringIO\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    print(\"[INFO] Extracting arguements\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument('--n_estimators', type=int, default=100)\n",
    "    parser.add_argument('--random_state', type=int, default=0)\n",
    "\n",
    "    # Data, model, and output_directories\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])\n",
    "    parser.add_argument('--train-file', type=str, default='train-V-1.csv')\n",
    "    parser.add_argument('--test-file', type=str, default='test-V-1.csv')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    print(\"SKLearn Version: \", sklearn.__version__)\n",
    "    print(\"Joblib Version: \", joblib.__version__)\n",
    "\n",
    "    print(\"[INFO] Reading data\")\n",
    "    print()\n",
    "    train_df = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    test_df = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "    features = list(train_df.columns)\n",
    "    label = features.pop(-1)\n",
    "\n",
    "    print(\"Building training and testing datasets\")\n",
    "    X_train = train_df[features]\n",
    "    X_test = train_df[label]\n",
    "    y_train = train_df[label]\n",
    "    y_test - test_df[label]\n",
    "\n",
    "    print(\"Training model\")\n",
    "    model = RandomForestClassifier(n_estimators=args.n_estimators, random_state=args.random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    model_path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, model_path)\n",
    "    print(\"Model persisted at \", model_path)\n",
    "    print()\n",
    "\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    test_acc = accuracy_score(y_test, y_pred_test)\n",
    "    test_rep = classification_report(y_test, y_pred_test)\n",
    "\n",
    "    print()\n",
    "    print(\"--- METRIC RESULTS ---\")\n",
    "    print(\"[TESTING] Model Accuracy is: \", test_acc)\n",
    "    print(\"[TESTING] Testing Report: \")\n",
    "    print(test_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"script.py\",\n",
    "    role=,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name=\"RF-custom-sklearn\",\n",
    "    hyperparameters={\n",
    "        \"n_estimators\": 100,\n",
    "        \"random_state\": 0,\n",
    "    },\n",
    "    use_spot_instance=True,\n",
    "    #max_wait=7200,\n",
    "    #max_run=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_estimator.fit({\"train\": trainpath, \"test\": testpath}, wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker_mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
