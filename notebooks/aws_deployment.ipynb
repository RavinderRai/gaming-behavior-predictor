{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying to AWS\n",
    "\n",
    "This notebook contains scripts that we will deploy to AWS, starting with the original dataset already in an S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's manage our imports and system paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "\n",
    "# Change to root directory\n",
    "os.chdir('..')\n",
    "\n",
    "# Create a folder for all our code\n",
    "SRC_PATH = Path(\"src\")\n",
    "sys.path.extend([f\"./{SRC_PATH}\"])\n",
    "\n",
    "# And we'll need our role's\n",
    "glue_role = os.getenv('GLUE_ROLE')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE')\n",
    "bucket = os.getenv('BUCKET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glue\n",
    "\n",
    "### ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(SRC_PATH / \"etl\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{SRC_PATH}/etl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/etl/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SRC_PATH}/etl/script.py\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import boto3\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'INPUT_BUCKET', 'INPUT_KEY', 'OUTPUT_BUCKET', 'OUTPUT_KEY'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Read data from S3\n",
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=args['INPUT_BUCKET'], Key=args['INPUT_KEY'])\n",
    "df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
    "\n",
    "#target label encoding\n",
    "df['EngagementLevel'] = df['EngagementLevel'].map({'Low': 0, 'Medium': 1, 'High': 2})\n",
    "\n",
    "# Perform transformations to independent variables\n",
    "df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n",
    "df['GameDifficulty'] = df['GameDifficulty'].map({'Easy': 0, 'Medium': 1, 'Hard': 2})\n",
    "df_encoded = pd.get_dummies(df, columns=['Location', 'GameGenre'], drop_first=True)\n",
    "\n",
    "encoded_cols = list(set(df_encoded.columns) - set(df.columns))\n",
    "df_encoded[encoded_cols] = df_encoded[encoded_cols].astype(int)\n",
    "\n",
    "# Convert the DataFrame back to CSV\n",
    "csv_buffer = StringIO()\n",
    "df_encoded.to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Upload the transformed data to S3\n",
    "s3_client.put_object(Bucket=args['OUTPUT_BUCKET'], Key=args['OUTPUT_KEY'], Body=csv_buffer.getvalue())\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script uploaded to s3://gaming-behavior/glue-scripts/script.py\n"
     ]
    }
   ],
   "source": [
    "file_path = f\"{(SRC_PATH / 'etl' / 'script.py').as_posix()}\"\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = 'gaming-behavior'\n",
    "script_file_name = 'script.py'\n",
    "s3_key = f'glue-scripts/{script_file_name}'\n",
    "\n",
    "# Upload the script to S3\n",
    "s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "print(f'Script uploaded to s3://{bucket_name}/{s3_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue job etl-job created successfully\n"
     ]
    }
   ],
   "source": [
    "glue_client = boto3.client('glue')\n",
    "\n",
    "# Parameters for the Glue job\n",
    "job_name = 'etl-job'\n",
    "script_location = f's3://{bucket_name}/{s3_key}'\n",
    "\n",
    "# S3 locations for input and output data\n",
    "input_bucket = 'gaming-behavior'\n",
    "input_key = 'raw_data/online_gaming_behavior_dataset.csv'\n",
    "output_bucket = 'gaming-behavior'\n",
    "output_key = 'transformed_data/transformed_online_gaming_behavior_dataset.csv'\n",
    "\n",
    "# Create or update the Glue job\n",
    "response = glue_client.create_job(\n",
    "    Name=job_name,\n",
    "    Role=glue_role,\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': script_location,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python',\n",
    "        '--enable-continuous-cloudwatch-log': 'true',\n",
    "        '--enable-spark-ui': 'true',\n",
    "        '--INPUT_BUCKET': input_bucket,\n",
    "        '--INPUT_KEY': input_key,\n",
    "        '--OUTPUT_BUCKET': output_bucket,\n",
    "        '--OUTPUT_KEY': output_key\n",
    "    },\n",
    "    MaxRetries=0,\n",
    "    MaxCapacity=2.0,\n",
    "    Timeout=2880,\n",
    "    GlueVersion='2.0'\n",
    ")\n",
    "\n",
    "print(f'Glue job {job_name} created successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue job etl-job started successfully with run ID: jr_d1cd5216b60496c48c2bc33ef3e33c44391d9a5e587b3d160ffaa58465a8e98c\n"
     ]
    }
   ],
   "source": [
    "start_response = glue_client.start_job_run(JobName=job_name)\n",
    "print(f'Glue job {job_name} started successfully with run ID: {start_response[\"JobRunId\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker\n",
    "\n",
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "S3_LOCATION = f\"s3://{bucket}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://gaming-behavior/transformed_data'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{S3_LOCATION}/transformed_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bucketgaming-behavior\n"
     ]
    }
   ],
   "source": [
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "pipeline_session = PipelineSession(default_bucket=bucket)\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "print(\"Using bucket\" + bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "(SRC_PATH / \"preprocessing\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{SRC_PATH}/preprocessing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('online_gaming_behavior_dataset.csv')\n",
    "\n",
    "#target label encoding\n",
    "df['EngagementLevel'] = df['EngagementLevel'].map({'Low': 0, 'Medium': 1, 'High': 2})\n",
    "\n",
    "# Perform transformations to independent variables\n",
    "df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n",
    "df['GameDifficulty'] = df['GameDifficulty'].map({'Easy': 0, 'Medium': 1, 'Hard': 2})\n",
    "df_encoded = pd.get_dummies(df, columns=['Location', 'GameGenre'], drop_first=True)\n",
    "\n",
    "encoded_cols = list(set(df_encoded.columns) - set(df.columns))\n",
    "df_encoded[encoded_cols] = df_encoded[encoded_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_encoded = df_encoded.drop(columns=['PlayerID'])\n",
    "df_train, df_test = train_test_split(df_encoded, test_size=0.2)\n",
    "\n",
    "y_train = df_train.EngagementLevel\n",
    "y_test = df_test.EngagementLevel\n",
    "\n",
    "X_train = df_train.drop(\"EngagementLevel\", axis=1)\n",
    "X_test = df_test.drop(\"EngagementLevel\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/preprocessing/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SRC_PATH}/preprocessing/script.py\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(base_directory):\n",
    "    \"\"\"Load the supplied data, split it and transform it.\"\"\"\n",
    "    df = _read_data_from_input_csv_files(base_directory)\n",
    "\n",
    "    # the only transformation we need to do is drop the player id and split the data\n",
    "    # everything else was done in the etl script\n",
    "    \n",
    "    df.drop(columns=['PlayerID'])\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "    y_train = df_train.EngagementLevel\n",
    "    y_test = df_test.EngagementLevel\n",
    "\n",
    "    X_train = df_train.drop(\"EngagementLevel\", axis=1)\n",
    "    X_test = df_test.drop(\"EngagementLevel\", axis=1)\n",
    "\n",
    "    _save_splits(base_directory, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def _read_data_from_input_csv_files(base_directory):\n",
    "    \"\"\"Read the data from the input CSV files.\n",
    "\n",
    "    This function reads every CSV file available and\n",
    "    concatenates them into a single dataframe.\n",
    "    \"\"\"\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = list(input_directory.glob(\"*.csv\"))\n",
    "\n",
    "    if len(files) == 0:\n",
    "        message = f\"The are no CSV files in {input_directory.as_posix()}/\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "    df = pd.concat(raw_data)\n",
    "\n",
    "    # Shuffle the data\n",
    "    return df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "def _save_splits(base_directory, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Save data splits to disk.\n",
    "\n",
    "    This function concatenates the transformed features\n",
    "    and the target variable, and saves each one of the split\n",
    "    sets to disk.\n",
    "    \"\"\"\n",
    "    train = pd.concat([X_train, y_train], axis=1)\n",
    "    test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(base_directory=\"/opt/ml/processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True)\n",
    "\n",
    "dataset_location = ParameterString(\n",
    "    name=\"dataset_location\",\n",
    "    default_value=f\"{S3_LOCATION}/transformed_data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    base_job_name=\"preprocess-data\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    role=sagemaker_role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\site-packages\\sagemaker\\workflow\\pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"preprocess-data\",\n",
    "    step_args=processor.run(\n",
    "        code=f\"{(SRC_PATH / 'preprocessing' / 'script.py').as_posix()}\",\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=dataset_location,\n",
    "                destination=\"/opt/ml/processing/input\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train\",\n",
    "                source=\"/opt/ml/processing/train\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test\",\n",
    "                source=\"/opt/ml/processing/test\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test\",\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:590184030535:pipeline/preprocessing-pipeline-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': '4b37ea0b-3e7e-48f4-b74d-95db0f20eb76',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '4b37ea0b-3e7e-48f4-b74d-95db0f20eb76',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '99',\n",
       "   'date': 'Wed, 07 Aug 2024 00:25:47 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    name=\"preprocessing-pipeline-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "preprocessing_pipeline.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:590184030535:pipeline/preprocessing-pipeline-pipeline/execution/g6zfmmlh822i', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x0000025DB778D190>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "(SRC_PATH / \"modeling\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{SRC_PATH}/modeling\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/modeling/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SRC_PATH}/modeling/script.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, \n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import tarfile\n",
    "\n",
    "\n",
    "\n",
    "def train(model_directory, train_path, test_path, pipeline_path, learning_rate=0.1, max_depth=7,):\n",
    "    print(f\"Keras version: {keras.__version__}\")\n",
    "\n",
    "    X_train = pd.read_csv(Path(train_path) / \"train.csv\")\n",
    "    y_train = X_train[X_train.columns[-1]]\n",
    "    X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "\n",
    "    X_test = pd.read_csv(Path(test_path) / \"test.csv\")\n",
    "    y_test = X_test[X_test.columns[-1]]\n",
    "    X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "    model = XGBClassifier(objective='multi:softmax', num_class=3, eval_metric='mlogloss', learning_rate=learning_rate, max_depth=max_depth)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    model_path = os.path.join(model_directory, \"model.joblib\")\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    print(\"[INFO] Extracting arguements\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "    parser.add_argument('--max_depth', type=int, default=7)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    train(\n",
    "        model_directory=os.environ[\"SM_MODEL_DIR\"],\n",
    "        train_path=os.environ[\"SM_CHANNEL_TRAIN\"],\n",
    "        validation_path=os.environ[\"SM_CHANNEL_TEST\"],\n",
    "        pipeline_path=os.environ[\"SM_CHANNEL_PIPELINE\"],\n",
    "        learning_rate=args.learning_rate,\n",
    "        max_depth=args.max_depth,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.t3.medium.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.xgboost import XGBoost\n",
    "\n",
    "estimator = XGBoost(\n",
    "    entry_point=\"script.py\",\n",
    "    source_dir=f\"{(SRC_PATH / 'modeling').as_posix()}\",\n",
    "    hyperparameters={\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_depth\": 7,\n",
    "    },\n",
    "    framework_version=\"1.2-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    role=sagemaker_role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: sagemaker-xgboost-2024-08-07-02-02-38-054\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Object of type Properties is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 23\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a SageMaker TrainingStep using the provided estimator.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TrainingStep(\n\u001b[0;32m      4\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain-model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m         step_args\u001b[38;5;241m=\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m         )\n\u001b[0;32m     21\u001b[0m     )\n\u001b[1;32m---> 23\u001b[0m train_model_step \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_training_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[64], line 5\u001b[0m, in \u001b[0;36mcreate_training_step\u001b[1;34m(estimator)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_training_step\u001b[39m(estimator):\n\u001b[0;32m      2\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a SageMaker TrainingStep using the provided estimator.\"\"\"\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TrainingStep(\n\u001b[0;32m      4\u001b[0m         name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain-model\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m----> 5\u001b[0m         step_args\u001b[38;5;241m=\u001b[39m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainingInput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                    \u001b[49m\u001b[43ms3_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessing_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcessingOutputConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOutputs\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS3Output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS3Uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext/csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mTrainingInput\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[43ms3_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreprocessing_step\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproperties\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcessingOutputConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOutputs\u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS3Output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mS3Uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcontent_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext/csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m                \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\site-packages\\sagemaker\\workflow\\pipeline_context.py:346\u001b[0m, in \u001b[0;36mrunnable_by_pipeline.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _StepArguments(retrieve_caller_name(self_instance), run_func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\site-packages\\sagemaker\\estimator.py:1347\u001b[0m, in \u001b[0;36mEstimatorBase.fit\u001b[1;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[0;32m   1344\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_for_training(job_name\u001b[38;5;241m=\u001b[39mjob_name)\n\u001b[0;32m   1346\u001b[0m experiment_config \u001b[38;5;241m=\u001b[39m check_and_get_run_experiment_config(experiment_config)\n\u001b[1;32m-> 1347\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job \u001b[38;5;241m=\u001b[39m \u001b[43m_TrainingJob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiment_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjobs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatest_training_job)\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\site-packages\\sagemaker\\estimator.py:2468\u001b[0m, in \u001b[0;36m_TrainingJob.start_new\u001b[1;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[0;32m   2465\u001b[0m train_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_train_args(estimator, inputs, experiment_config)\n\u001b[0;32m   2467\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain args after processing defaults: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, train_args)\n\u001b[1;32m-> 2468\u001b[0m \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrain_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(estimator\u001b[38;5;241m.\u001b[39msagemaker_session, estimator\u001b[38;5;241m.\u001b[39m_current_job_name)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\site-packages\\sagemaker\\session.py:1040\u001b[0m, in \u001b[0;36mSession.train\u001b[1;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, training_image_config, infra_check_config, container_entry_point, container_arguments, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy, remote_debug_config, session_chaining_config)\u001b[0m\n\u001b[0;32m   1037\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, json\u001b[38;5;241m.\u001b[39mdumps(request, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m))\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_training_job(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest)\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_create_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\site-packages\\sagemaker\\session.py:6514\u001b[0m, in \u001b[0;36mSession._intercept_create_request\u001b[1;34m(self, request, create, func_name)\u001b[0m\n\u001b[0;32m   6497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_intercept_create_request\u001b[39m(\n\u001b[0;32m   6498\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   6499\u001b[0m     request: typing\u001b[38;5;241m.\u001b[39mDict,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6502\u001b[0m     \u001b[38;5;66;03m# pylint: disable=unused-argument\u001b[39;00m\n\u001b[0;32m   6503\u001b[0m ):\n\u001b[0;32m   6504\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"This function intercepts the create job request.\u001b[39;00m\n\u001b[0;32m   6505\u001b[0m \n\u001b[0;32m   6506\u001b[0m \u001b[38;5;124;03m    PipelineSession inherits this Session class and will override\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6512\u001b[0m \u001b[38;5;124;03m        func_name (str): the name of the function needed intercepting\u001b[39;00m\n\u001b[0;32m   6513\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\site-packages\\sagemaker\\session.py:1037\u001b[0m, in \u001b[0;36mSession.train.<locals>.submit\u001b[1;34m(request)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msubmit\u001b[39m(request):\n\u001b[0;32m   1036\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating training-job with name: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, job_name)\n\u001b[1;32m-> 1037\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msagemaker_client\u001b[38;5;241m.\u001b[39mcreate_training_job(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrequest)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\json\\__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONEncoder\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskipkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_ascii\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheck_circular\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_circular\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseparators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseparators\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\json\\encoder.py:201\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    199\u001b[0m chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterencode(o, _one_shot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(chunks, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 201\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(chunks)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\json\\encoder.py:431\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[0;32m    430\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 431\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n\u001b[0;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    433\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\json\\encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\json\\encoder.py:325\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[1;34m(lst, _current_indent_level)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 325\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\json\\encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\json\\encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\json\\encoder.py:405\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_dict\u001b[1;34m(dct, _current_indent_level)\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    404\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 405\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    407\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\json\\encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCircular reference detected\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    437\u001b[0m     markers[markerid] \u001b[38;5;241m=\u001b[39m o\n\u001b[1;32m--> 438\u001b[0m o \u001b[38;5;241m=\u001b[39m \u001b[43m_default\u001b[49m\u001b[43m(\u001b[49m\u001b[43mo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m markers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault\u001b[39m(\u001b[38;5;28mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;124;03m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;124;03m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mObject of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mo\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis not JSON serializable\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type Properties is not JSON serializable"
     ]
    }
   ],
   "source": [
    "def create_training_step(estimator):\n",
    "    \"\"\"Create a SageMaker TrainingStep using the provided estimator.\"\"\"\n",
    "    return TrainingStep(\n",
    "        name=\"train-model\",\n",
    "        step_args=estimator.fit(\n",
    "            inputs={\n",
    "                \"train\": TrainingInput(\n",
    "                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                        \"train\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                ),\n",
    "                \"test\": TrainingInput(\n",
    "                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                        \"test\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                )\n",
    "            },\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_model_step = create_training_step(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "train_pipeline = Pipeline(\n",
    "    name=\"train-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "        train_model_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "train_pipeline.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker_mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
