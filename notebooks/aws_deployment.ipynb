{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying to AWS\n",
    "\n",
    "This notebook contains scripts that we will deploy to AWS, starting with the original dataset already in an S3 bucket."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's manage our imports and system paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "\n",
    "# Change to root directory\n",
    "os.chdir('..')\n",
    "\n",
    "# Create a folder for all our code\n",
    "SRC_PATH = Path(\"src\")\n",
    "sys.path.extend([f\"./{SRC_PATH}\"])\n",
    "\n",
    "# And we'll need our role's\n",
    "glue_role = os.getenv('GLUE_ROLE')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE')\n",
    "bucket = os.getenv('BUCKET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glue\n",
    "\n",
    "### ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "(SRC_PATH / \"etl\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{SRC_PATH}/etl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/etl/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SRC_PATH}/etl/script.py\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import boto3\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'INPUT_BUCKET', 'INPUT_KEY', 'OUTPUT_BUCKET', 'OUTPUT_KEY'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Read data from S3\n",
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=args['INPUT_BUCKET'], Key=args['INPUT_KEY'])\n",
    "df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
    "\n",
    "#target label encoding\n",
    "df['EngagementLevel'] = df['EngagementLevel'].map({'Low': 0, 'Medium': 1, 'High': 2})\n",
    "\n",
    "# Perform transformations to independent variables\n",
    "df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n",
    "df['GameDifficulty'] = df['GameDifficulty'].map({'Easy': 0, 'Medium': 1, 'Hard': 2})\n",
    "df_encoded = pd.get_dummies(df, columns=['Location', 'GameGenre'], drop_first=True)\n",
    "\n",
    "encoded_cols = list(set(df_encoded.columns) - set(df.columns))\n",
    "df_encoded[encoded_cols] = df_encoded[encoded_cols].astype(int)\n",
    "\n",
    "# Convert the DataFrame back to CSV\n",
    "csv_buffer = StringIO()\n",
    "df_encoded.to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Upload the transformed data to S3\n",
    "s3_client.put_object(Bucket=args['OUTPUT_BUCKET'], Key=args['OUTPUT_KEY'], Body=csv_buffer.getvalue())\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script uploaded to s3://gaming-behavior/glue-scripts/script.py\n"
     ]
    }
   ],
   "source": [
    "file_path = f\"{(SRC_PATH / 'etl' / 'script.py').as_posix()}\"\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = 'gaming-behavior'\n",
    "script_file_name = 'script.py'\n",
    "s3_key = f'glue-scripts/{script_file_name}'\n",
    "\n",
    "# Upload the script to S3\n",
    "s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "print(f'Script uploaded to s3://{bucket_name}/{s3_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue job etl-job created successfully\n"
     ]
    }
   ],
   "source": [
    "glue_client = boto3.client('glue')\n",
    "\n",
    "# Parameters for the Glue job\n",
    "job_name = 'etl-job'\n",
    "script_location = f's3://{bucket_name}/{s3_key}'\n",
    "\n",
    "# S3 locations for input and output data\n",
    "input_bucket = 'gaming-behavior'\n",
    "input_key = 'raw_data/online_gaming_behavior_dataset.csv'\n",
    "output_bucket = 'gaming-behavior'\n",
    "output_key = 'transformed_data/transformed_online_gaming_behavior_dataset.csv'\n",
    "\n",
    "# Create or update the Glue job\n",
    "response = glue_client.create_job(\n",
    "    Name=job_name,\n",
    "    Role=glue_role,\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': script_location,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python',\n",
    "        '--enable-continuous-cloudwatch-log': 'true',\n",
    "        '--enable-spark-ui': 'true',\n",
    "        '--INPUT_BUCKET': input_bucket,\n",
    "        '--INPUT_KEY': input_key,\n",
    "        '--OUTPUT_BUCKET': output_bucket,\n",
    "        '--OUTPUT_KEY': output_key\n",
    "    },\n",
    "    MaxRetries=0,\n",
    "    MaxCapacity=2.0,\n",
    "    Timeout=2880,\n",
    "    GlueVersion='2.0'\n",
    ")\n",
    "\n",
    "print(f'Glue job {job_name} created successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue job etl-job started successfully with run ID: jr_d1cd5216b60496c48c2bc33ef3e33c44391d9a5e587b3d160ffaa58465a8e98c\n"
     ]
    }
   ],
   "source": [
    "start_response = glue_client.start_job_run(JobName=job_name)\n",
    "print(f'Glue job {job_name} started successfully with run ID: {start_response[\"JobRunId\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker\n",
    "\n",
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://gaming-behavior/transformed_data'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "S3_LOCATION = f\"s3://{bucket}/transformed_data\"\n",
    "S3_LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bucketgaming-behavior\n"
     ]
    }
   ],
   "source": [
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "pipeline_session = PipelineSession(default_bucket=bucket)\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "print(\"Using bucket\" + bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "(SRC_PATH / \"preprocessing\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{SRC_PATH}/preprocessing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/preprocessing/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SRC_PATH}/preprocessing/script.py\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(base_directory):\n",
    "    \"\"\"Load the supplied data, split it and transform it.\"\"\"\n",
    "    df = _read_data_from_input_csv_files(base_directory)\n",
    "\n",
    "    # the only transformation we need to do is drop the player id and split the data\n",
    "    # everything else was done in the etl script\n",
    "    \n",
    "    df.drop(columns=['PlayerID'])\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "    y_train = df_train.EngagementLevel\n",
    "    y_test = df_test.EngagementLevel\n",
    "\n",
    "    X_train = df_train.drop(\"EngagementLevel\", axis=1)\n",
    "    X_test = df_test.drop(\"EngagementLevel\", axis=1)\n",
    "\n",
    "    _save_splits(base_directory, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def _read_data_from_input_csv_files(base_directory):\n",
    "    \"\"\"Read the data from the input CSV files.\n",
    "\n",
    "    This function reads every CSV file available and\n",
    "    concatenates them into a single dataframe.\n",
    "    \"\"\"\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = list(input_directory.glob(\"*.csv\"))\n",
    "\n",
    "    if len(files) == 0:\n",
    "        message = f\"The are no CSV files in {input_directory.as_posix()}/\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "    df = pd.concat(raw_data)\n",
    "\n",
    "    # Shuffle the data\n",
    "    return df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "def _save_splits(base_directory, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Save data splits to disk.\n",
    "\n",
    "    This function concatenates the transformed features\n",
    "    and the target variable, and saves each one of the split\n",
    "    sets to disk.\n",
    "    \"\"\"\n",
    "    train = np.concatenate((X_train, y_train), axis=1)\n",
    "    test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=False, index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(base_directory=\"/opt/ml/processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "\n",
    "pipeline_definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True)\n",
    "\n",
    "dataset_location = ParameterString(\n",
    "    name=\"dataset_location\",\n",
    "    default_value=S3_LOCATION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    base_job_name=\"preprocess-data\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    role=sagemaker_role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\site-packages\\sagemaker\\workflow\\pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"preprocess-data\",\n",
    "    step_args=processor.run(\n",
    "        code=f\"{(SRC_PATH / 'preprocessing' / 'script.py').as_posix()}\",\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=dataset_location,\n",
    "                destination=\"/opt/ml/processing/input\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train\",\n",
    "                source=\"/opt/ml/processing/train\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test\",\n",
    "                source=\"/opt/ml/processing/test\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test\",\n",
    "            )\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:590184030535:pipeline/preprocessing-pipeline-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': '2019af45-e627-40c9-a6e0-75f682124f61',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '2019af45-e627-40c9-a6e0-75f682124f61',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '99',\n",
       "   'date': 'Tue, 06 Aug 2024 02:20:21 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    name=\"preprocessing-pipeline-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "preprocessing_pipeline.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:590184030535:pipeline/preprocessing-pipeline-pipeline/execution/sdjxq8w4yiz1', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x000001335F0B8B20>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "(SRC_PATH / \"modeling\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{SRC_PATH}/modeling\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/modeling/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SRC_PATH}/modeling/script.py\n",
    "\n",
    "from sklearn.ensemble import  RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, precision_score\n",
    "import sklearn\n",
    "import joblib\n",
    "import boto3\n",
    "import pathlib\n",
    "from io import  StringIO\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    print(\"[INFO] Extracting arguements\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "    parser.add_argument('--max_depth', type=int, default=7)\n",
    "\n",
    "    # Data, model, and output_directories\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])\n",
    "    parser.add_argument('--train-file', type=str, default='train-V-1.csv')\n",
    "    parser.add_argument('--test-file', type=str, default='test-V-1.csv')\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "\n",
    "    print(\"[INFO] Reading data\")\n",
    "    print()\n",
    "    df_train = pd.read_csv(os.path.join(args.train, args.train_file))\n",
    "    df_test = pd.read_csv(os.path.join(args.test, args.test_file))\n",
    "\n",
    "    features = list(df_train.columns)\n",
    "    label = features.pop(-1)\n",
    "\n",
    "    print(\"Building training and testing datasets\")\n",
    "    X_train = df_train[features]\n",
    "    X_test = df_test[label]\n",
    "    y_train = df_train[label]\n",
    "    y_test - df_test[label]\n",
    "\n",
    "    print(\"Training model\")\n",
    "    model = XGBClassifier(objective='multi:softmax', num_class=3, eval_metric='mlogloss', learning_rate=0.1, max_depth=7)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    model_path = os.path.join(args.model_dir, \"model.joblib\")\n",
    "    joblib.dump(model, model_path)\n",
    "    print(\"Model persisted at \", model_path)\n",
    "    print()\n",
    "    print(\"--- METRIC RESULTS ---\")\n",
    "    print(\"[TESTING] Model Accuracy is: \", accuracy)\n",
    "    print(\"[TESTING] Kappa Score is: \", kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sk_prefix = \"sagemaker/mobile_price_classification/sklearncontainer\"\n",
    "trainpath = sess.upload_data(\n",
    "    path=\"train-V-1.csv\", bucket=bucket, key_prefix=sk_prefix\n",
    ")\n",
    "\n",
    "testpath = sess.upload_data(\n",
    "    path=\"test-V-1.csv\", bucket=bucket, key_prefix=sk_prefix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "\n",
    "sklearn_estimator = SKLearn(\n",
    "    entry_point=\"script.py\",\n",
    "    role=sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    base_job_name=\"RF-custom-sklearn\",\n",
    "    hyperparameters={\n",
    "        \"n_estimators\": 100,\n",
    "        \"random_state\": 0,\n",
    "    },\n",
    "    use_spot_instance=True,\n",
    "    #max_wait=7200,\n",
    "    #max_run=3600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_estimator.fit({\"train\": trainpath, \"test\": testpath}, wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker_mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
