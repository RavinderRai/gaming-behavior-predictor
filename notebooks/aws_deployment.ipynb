{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying to AWS\n",
    "\n",
    "This notebook contains scripts that we will deploy to AWS, starting with the original dataset already in an S3 bucket. The project itself is about detecting three different gaming behaviors using an XGBoost classifier. We will do data cleaning and run that process with AWS Glue, then do any data preprocessing left in a preprocessing script with Sagemaker. We will afterwards of course do model training in sagemaker, and start a pipeline there just for demonstration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's manage our imports and system paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import boto3\n",
    "from pathlib import Path\n",
    "\n",
    "# Change to root directory\n",
    "os.chdir('..')\n",
    "\n",
    "# Create a folder for all our code\n",
    "SRC_PATH = Path(\"src\")\n",
    "sys.path.extend([f\"./{SRC_PATH}\"])\n",
    "\n",
    "# And we'll need our role's\n",
    "glue_role = os.getenv('GLUE_ROLE')\n",
    "sagemaker_role = os.getenv('SAGEMAKER_ROLE')\n",
    "bucket = os.getenv('BUCKET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glue\n",
    "\n",
    "### ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(SRC_PATH / \"etl\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{SRC_PATH}/etl\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/etl/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SRC_PATH}/etl/script.py\n",
    "\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import boto3\n",
    "\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME', 'INPUT_BUCKET', 'INPUT_KEY', 'OUTPUT_BUCKET', 'OUTPUT_KEY'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Read data from S3\n",
    "s3_client = boto3.client('s3')\n",
    "obj = s3_client.get_object(Bucket=args['INPUT_BUCKET'], Key=args['INPUT_KEY'])\n",
    "df = pd.read_csv(StringIO(obj['Body'].read().decode('utf-8')))\n",
    "\n",
    "#target label encoding\n",
    "df['EngagementLevel'] = df['EngagementLevel'].map({'Low': 0, 'Medium': 1, 'High': 2})\n",
    "\n",
    "# Perform transformations to independent variables\n",
    "df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n",
    "df['GameDifficulty'] = df['GameDifficulty'].map({'Easy': 0, 'Medium': 1, 'Hard': 2})\n",
    "df_encoded = pd.get_dummies(df, columns=['Location', 'GameGenre'], drop_first=True)\n",
    "\n",
    "encoded_cols = list(set(df_encoded.columns) - set(df.columns))\n",
    "df_encoded[encoded_cols] = df_encoded[encoded_cols].astype(int)\n",
    "\n",
    "# Convert the DataFrame back to CSV\n",
    "csv_buffer = StringIO()\n",
    "df_encoded.to_csv(csv_buffer, index=False)\n",
    "\n",
    "# Upload the transformed data to S3\n",
    "s3_client.put_object(Bucket=args['OUTPUT_BUCKET'], Key=args['OUTPUT_KEY'], Body=csv_buffer.getvalue())\n",
    "\n",
    "job.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script uploaded to s3://gaming-behavior/glue-scripts/script.py\n"
     ]
    }
   ],
   "source": [
    "file_path = f\"{(SRC_PATH / 'etl' / 'script.py').as_posix()}\"\n",
    "s3_client = boto3.client('s3')\n",
    "bucket_name = 'gaming-behavior'\n",
    "script_file_name = 'script.py'\n",
    "s3_key = f'glue-scripts/{script_file_name}'\n",
    "\n",
    "# Upload the script to S3\n",
    "s3_client.upload_file(file_path, bucket_name, s3_key)\n",
    "print(f'Script uploaded to s3://{bucket_name}/{s3_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue job etl-job created successfully\n"
     ]
    }
   ],
   "source": [
    "glue_client = boto3.client('glue')\n",
    "\n",
    "# Parameters for the Glue job\n",
    "job_name = 'etl-job'\n",
    "script_location = f's3://{bucket_name}/{s3_key}'\n",
    "\n",
    "# S3 locations for input and output data\n",
    "input_bucket = 'gaming-behavior'\n",
    "input_key = 'raw_data/online_gaming_behavior_dataset.csv'\n",
    "output_bucket = 'gaming-behavior'\n",
    "output_key = 'transformed_data/transformed_online_gaming_behavior_dataset.csv'\n",
    "\n",
    "# Create or update the Glue job\n",
    "response = glue_client.create_job(\n",
    "    Name=job_name,\n",
    "    Role=glue_role,\n",
    "    Command={\n",
    "        'Name': 'glueetl',\n",
    "        'ScriptLocation': script_location,\n",
    "        'PythonVersion': '3'\n",
    "    },\n",
    "    DefaultArguments={\n",
    "        '--job-language': 'python',\n",
    "        '--enable-continuous-cloudwatch-log': 'true',\n",
    "        '--enable-spark-ui': 'true',\n",
    "        '--INPUT_BUCKET': input_bucket,\n",
    "        '--INPUT_KEY': input_key,\n",
    "        '--OUTPUT_BUCKET': output_bucket,\n",
    "        '--OUTPUT_KEY': output_key\n",
    "    },\n",
    "    MaxRetries=0,\n",
    "    MaxCapacity=2.0,\n",
    "    Timeout=2880,\n",
    "    GlueVersion='2.0'\n",
    ")\n",
    "\n",
    "print(f'Glue job {job_name} created successfully')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glue job etl-job started successfully with run ID: jr_195ff9faba49f0f2b2324e1d2fa90e9672d0b305ba6b92941a48b8b49e9eb36d\n"
     ]
    }
   ],
   "source": [
    "start_response = glue_client.start_job_run(JobName=job_name)\n",
    "print(f'Glue job {job_name} started successfully with run ID: {start_response[\"JobRunId\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker\n",
    "\n",
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.workflow.parameters import ParameterString\n",
    "from sagemaker.workflow.pipeline_definition_config import PipelineDefinitionConfig\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.steps import CacheConfig\n",
    "\n",
    "# don't redo steps if already done from previous failed jobs\n",
    "cache_config = CacheConfig(enable_caching=True, expire_after=\"15d\")\n",
    "\n",
    "S3_LOCATION = f\"s3://{bucket}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://gaming-behavior/transformed_data'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"{S3_LOCATION}/transformed_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using bucketgaming-behavior\n"
     ]
    }
   ],
   "source": [
    "sm_boto3 = boto3.client(\"sagemaker\")\n",
    "pipeline_session = PipelineSession(default_bucket=bucket)\n",
    "sagemaker_session = sagemaker.session.Session()\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "print(\"Using bucket\" + bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(SRC_PATH / \"preprocessing\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{SRC_PATH}/preprocessing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('online_gaming_behavior_dataset.csv')\n",
    "\n",
    "#target label encoding\n",
    "df['EngagementLevel'] = df['EngagementLevel'].map({'Low': 0, 'Medium': 1, 'High': 2})\n",
    "\n",
    "# Perform transformations to independent variables\n",
    "df['Gender'] = df['Gender'].map({'Male': 1, 'Female': 0})\n",
    "df['GameDifficulty'] = df['GameDifficulty'].map({'Easy': 0, 'Medium': 1, 'Hard': 2})\n",
    "df_encoded = pd.get_dummies(df, columns=['Location', 'GameGenre'], drop_first=True)\n",
    "\n",
    "encoded_cols = list(set(df_encoded.columns) - set(df.columns))\n",
    "df_encoded[encoded_cols] = df_encoded[encoded_cols].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_encoded = df_encoded.drop(columns=['PlayerID'])\n",
    "df_train, df_test = train_test_split(df_encoded, test_size=0.2)\n",
    "\n",
    "y_train = df_train.EngagementLevel\n",
    "y_test = df_test.EngagementLevel\n",
    "\n",
    "X_train = df_train.drop(\"EngagementLevel\", axis=1)\n",
    "X_test = df_test.drop(\"EngagementLevel\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.concat([X_train, y_train], axis=1)\n",
    "test = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/preprocessing/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SRC_PATH}/preprocessing/script.py\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(base_directory):\n",
    "    \"\"\"Load the supplied data, split it and transform it.\"\"\"\n",
    "    df = _read_data_from_input_csv_files(base_directory)\n",
    "\n",
    "    # the only transformation we need to do is drop the player id and split the data\n",
    "    # everything else was done in the etl script\n",
    "    \n",
    "    df.drop(columns=['PlayerID'])\n",
    "    df_train, df_test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "    y_train = df_train.EngagementLevel\n",
    "    y_test = df_test.EngagementLevel\n",
    "\n",
    "    X_train = df_train.drop(\"EngagementLevel\", axis=1)\n",
    "    X_test = df_test.drop(\"EngagementLevel\", axis=1)\n",
    "\n",
    "    _save_splits(base_directory, X_train, y_train, X_test, y_test)\n",
    "\n",
    "\n",
    "def _read_data_from_input_csv_files(base_directory):\n",
    "    \"\"\"Read the data from the input CSV files.\n",
    "\n",
    "    This function reads every CSV file available and\n",
    "    concatenates them into a single dataframe.\n",
    "    \"\"\"\n",
    "    input_directory = Path(base_directory) / \"input\"\n",
    "    files = list(input_directory.glob(\"*.csv\"))\n",
    "\n",
    "    if len(files) == 0:\n",
    "        message = f\"The are no CSV files in {input_directory.as_posix()}/\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "    df = pd.concat(raw_data)\n",
    "\n",
    "    # Shuffle the data\n",
    "    return df.sample(frac=1, random_state=42)\n",
    "\n",
    "\n",
    "def _save_splits(base_directory, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Save data splits to disk.\n",
    "\n",
    "    This function concatenates the transformed features\n",
    "    and the target variable, and saves each one of the split\n",
    "    sets to disk.\n",
    "    \"\"\"\n",
    "    train = pd.concat([X_train, y_train], axis=1)\n",
    "    test = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame(train).to_csv(train_path / \"train.csv\", header=True, index=False)\n",
    "    pd.DataFrame(test).to_csv(test_path / \"test.csv\", header=True, index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(base_directory=\"/opt/ml/processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_definition_config = PipelineDefinitionConfig(use_custom_job_prefix=True)\n",
    "\n",
    "dataset_location = ParameterString(\n",
    "    name=\"dataset_location\",\n",
    "    default_value=f\"{S3_LOCATION}/transformed_data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "\n",
    "processor = SKLearnProcessor(\n",
    "    base_job_name=\"preprocess-data\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    role=sagemaker_role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\site-packages\\sagemaker\\workflow\\pipeline_context.py:332: UserWarning: Running within a PipelineSession, there will be No Wait, No Logs, and No Job being started.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "preprocessing_step = ProcessingStep(\n",
    "    name=\"preprocess-data\",\n",
    "    step_args=processor.run(\n",
    "        code=f\"{(SRC_PATH / 'preprocessing' / 'script.py').as_posix()}\",\n",
    "        inputs=[\n",
    "            ProcessingInput(\n",
    "                source=dataset_location,\n",
    "                destination=\"/opt/ml/processing/input\",\n",
    "            ),\n",
    "        ],\n",
    "        outputs=[\n",
    "            ProcessingOutput(\n",
    "                output_name=\"train\",\n",
    "                source=\"/opt/ml/processing/train\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/train\",\n",
    "            ),\n",
    "            ProcessingOutput(\n",
    "                output_name=\"test\",\n",
    "                source=\"/opt/ml/processing/test\",\n",
    "                destination=f\"{S3_LOCATION}/preprocessing/test\",\n",
    "            )\n",
    "        ],\n",
    "    ),\n",
    "    cache_config=cache_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:590184030535:pipeline/preprocessing-pipeline-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': '7b4060b0-cc4a-4327-8cc2-68ba591581e4',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7b4060b0-cc4a-4327-8cc2-68ba591581e4',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '99',\n",
       "   'date': 'Fri, 09 Aug 2024 00:47:37 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "preprocessing_pipeline = Pipeline(\n",
    "    name=\"preprocessing-pipeline-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "preprocessing_pipeline.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:590184030535:pipeline/preprocessing-pipeline-pipeline/execution/oo19ki6bsmg4', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x000001853A493AC0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "(SRC_PATH / \"modeling\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{SRC_PATH}/modeling\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/modeling/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {SRC_PATH}/modeling/script.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import tarfile\n",
    "\n",
    "\n",
    "\n",
    "def train(model_directory, train_path, test_path, learning_rate=0.1, max_depth=7,):\n",
    "\n",
    "    X_train = pd.read_csv(Path(train_path) / \"train.csv\")\n",
    "    y_train = X_train[X_train.columns[-1]]\n",
    "    X_train = X_train.drop(X_train.columns[-1], axis=1)\n",
    "\n",
    "    X_test = pd.read_csv(Path(test_path) / \"test.csv\")\n",
    "    y_test = X_test[X_test.columns[-1]]\n",
    "    X_test = X_test.drop(X_test.columns[-1], axis=1)\n",
    "\n",
    "    model = xgb.XGBClassifier(objective='multi:softmax', num_class=3, eval_metric='mlogloss', learning_rate=learning_rate, max_depth=max_depth)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "    model_path = os.path.join(model_directory, \"model.joblib\")\n",
    "    joblib.dump(model, model_path)\n",
    "\n",
    "    with tarfile.open(os.path.join(model_directory, \"model.tar.gz\"), \"w:gz\") as tar:\n",
    "        tar.add(model_path, arcname=os.path.basename(model_path))\n",
    "\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "    print(\"[INFO] Extracting arguements\")\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "    parser.add_argument('--max_depth', type=int, default=7)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    train(\n",
    "        model_directory=os.environ[\"SM_MODEL_DIR\"],\n",
    "        train_path=os.environ[\"SM_CHANNEL_TRAIN\"],\n",
    "        test_path=os.environ[\"SM_CHANNEL_TEST\"],\n",
    "        learning_rate=args.learning_rate,\n",
    "        max_depth=args.max_depth,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.m5.xlarge.\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.workflow.steps import TrainingStep\n",
    "from sagemaker.xgboost import XGBoost\n",
    "\n",
    "estimator = XGBoost(\n",
    "    entry_point=\"script.py\",\n",
    "    source_dir=f\"{(SRC_PATH / 'modeling').as_posix()}\",\n",
    "    hyperparameters={\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"max_depth\": 7,\n",
    "    },\n",
    "    framework_version=\"1.2-1\",\n",
    "    py_version=\"py3\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    role=sagemaker_role,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_step(estimator):\n",
    "    \"\"\"Create a SageMaker TrainingStep using the provided estimator.\"\"\"\n",
    "    return TrainingStep(\n",
    "        name=\"train-model\",\n",
    "        step_args=estimator.fit(\n",
    "            inputs={\n",
    "                \"train\": TrainingInput(\n",
    "                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                        \"train\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                ),\n",
    "                \"test\": TrainingInput(\n",
    "                    s3_data=preprocessing_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                        \"test\"\n",
    "                    ].S3Output.S3Uri,\n",
    "                    content_type=\"text/csv\",\n",
    "                )\n",
    "            },\n",
    "        ),\n",
    "        cache_config=cache_config\n",
    "    )\n",
    "\n",
    "train_model_step = create_training_step(estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PipelineArn': 'arn:aws:sagemaker:us-east-1:590184030535:pipeline/train-pipeline',\n",
       " 'ResponseMetadata': {'RequestId': 'd5227536-d976-4e69-bc4c-e50b5abb3f33',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'd5227536-d976-4e69-bc4c-e50b5abb3f33',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '82',\n",
       "   'date': 'Fri, 09 Aug 2024 01:32:16 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "\n",
    "train_pipeline = Pipeline(\n",
    "    name=\"train-pipeline\",\n",
    "    parameters=[dataset_location],\n",
    "    steps=[\n",
    "        preprocessing_step,\n",
    "        train_model_step,\n",
    "    ],\n",
    "    pipeline_definition_config=pipeline_definition_config,\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "train_pipeline.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_PipelineExecution(arn='arn:aws:sagemaker:us-east-1:590184030535:pipeline/train-pipeline/execution/dsnhjapm4ky5', sagemaker_session=<sagemaker.workflow.pipeline_context.PipelineSession object at 0x000001853A493AC0>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.xgboost import XGBoostModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = f\"{S3_LOCATION}/sagemaker-xgboost-dsnhjapm4ky5-rcmJhiITEj/output/model.tar.gz\"\n",
    "\n",
    "xgboost_model = XGBoostModel(\n",
    "    model_data=model_data,\n",
    "    role=sagemaker_role,\n",
    "    framework_version=\"1.2-1\",\n",
    "    sagemaker_session=sagemaker.Session()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.m5.xlarge.\n",
      "INFO:sagemaker:Creating model with name: sagemaker-xgboost-2024-08-09-01-52-17-541\n",
      "INFO:sagemaker:Creating endpoint-config with name sagemaker-xgboost-2024-08-09-01-52-18-763\n",
      "INFO:sagemaker:Creating endpoint with name sagemaker-xgboost-2024-08-09-01-52-18-763\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!"
     ]
    }
   ],
   "source": [
    "# Deploy the model to a SageMaker endpoint\n",
    "predictor = xgboost_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.xlarge'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "\n",
    "file_key = 'preprocessing/test/test.csv'\n",
    "\n",
    "# Download the file content to a string\n",
    "csv_obj = s3.get_object(Bucket=bucket, Key=file_key)\n",
    "body = csv_obj['Body']\n",
    "csv_string = body.read().decode('utf-8')\n",
    "\n",
    "# Use StringIO to create a file-like object\n",
    "df = pd.read_csv(StringIO(csv_string))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df.drop(columns='EngagementLevel')\n",
    "sample = X_test.iloc[0].to_csv(header=False, index=False).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (502) from primary with message \"<html>\r\n<head><title>502 Bad Gateway</title></head>\r\n<body bgcolor=\"white\">\r\n<center><h1>502 Bad Gateway</h1></center>\r\n<hr><center>nginx/1.14.0 (Ubuntu)</center>\r\n</body>\r\n</html>\r\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/sagemaker-xgboost-2024-08-09-01-52-18-763 in account 590184030535 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m predictor\u001b[38;5;241m.\u001b[39mserializer \u001b[38;5;241m=\u001b[39m CSVSerializer()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\site-packages\\sagemaker\\base_predictor.py:212\u001b[0m, in \u001b[0;36mPredictor.predict\u001b[1;34m(self, data, initial_args, target_model, target_variant, inference_id, custom_attributes, component_name)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inference_component_name:\n\u001b[0;32m    210\u001b[0m     request_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInferenceComponentName\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m inference_component_name\n\u001b[1;32m--> 212\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msagemaker_runtime_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrequest_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_response(response)\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\site-packages\\botocore\\client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m     )\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[1;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\RaviB\\anaconda3\\envs\\sagemaker_mini\\lib\\site-packages\\botocore\\client.py:1017\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[1;34m(self, operation_name, api_params)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[0;32m   1014\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1015\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[1;32m-> 1017\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[0;32m   1018\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1019\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[1;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (502) from primary with message \"<html>\r\n<head><title>502 Bad Gateway</title></head>\r\n<body bgcolor=\"white\">\r\n<center><h1>502 Bad Gateway</h1></center>\r\n<hr><center>nginx/1.14.0 (Ubuntu)</center>\r\n</body>\r\n</html>\r\n\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/sagemaker-xgboost-2024-08-09-01-52-18-763 in account 590184030535 for more information."
     ]
    }
   ],
   "source": [
    "predictor.serializer = CSVSerializer()\n",
    "\n",
    "predictor.predict(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction\n",
    "input_data = '6.3,3.3,6.0,2.5'  # Example input\n",
    "response = predictor.predict(input_data)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting endpoint configuration with name: sagemaker-xgboost-2024-08-09-01-52-18-763\n",
      "INFO:sagemaker:Deleting endpoint with name: sagemaker-xgboost-2024-08-09-01-52-18-763\n"
     ]
    }
   ],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sagemaker_mini",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
